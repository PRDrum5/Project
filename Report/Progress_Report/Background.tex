\documentclass[12pt]{article}

\usepackage[nottoc,notlot,notlof]{tocbibind}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{geometry}
 \geometry
 {
     a4paper,
     left=25mm,
     right=25mm,
     top=30mm,
     bottom=30mm,
 }

\title{MSc Background Report}
\author{Peter Robertson}
\date{}

\begin{document}
\maketitle

% page numbering etc.
\pagenumbering{roman}
\setcounter{page}{0}
\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\tableofcontents 
\clearpage{\pagestyle{empty}\cleardoublepage}


\pagenumbering{arabic}
\setcounter{page}{1}

\section{Introduction}

\textbf{( I want to rewrite this )}

Current methods of visual speech prediction, often referred to as lip reading, focuses on training machine learning models on 2D temporal data of participants speaking, labelled with the accompanying spoken text \cite{Chung2016, Assael2016, Chung2017, Shillingford2018}.
However there has been little progress in lip reading from 3D temporal models which contain depth information of the subject.
A substantial barrier to this problem is the lack of 3D datasets of subjects speaking when compared to the relative abundance of video datasets.
To first attempt this problem, new datasets which capture 3D temporal models of subjects heads speaking must be established by the community.

\section{Lip Reading}
This section shall discuss current lip reading models built with deep learning techniques and the availability of both 2D and 3D temporal datasets which lip reading models can currently be trained on.
In recent years the problem of Lip Reading has seen huge advances due to the availability of new datasets and the use of Deep Learning models.
Due to the large availability of video data, datasets such as GRID \cite{Cooke2006}, LRW \cite{Chung2016}, LRS \cite{Chung2017} and LSVSR \cite{Shillingford2018} have been constructed by compiling the appropriate data.

Current lip reading models currently make use of 2D temporal data such as videos.
This is convenient as video data is already widely available and relatively straightforward to obtain and process.
Unlike 2D temporal data, 3D temporal data (3D video), requires multiple cameras to record simultaneously, which requires synchronisation, adding to the complexity of the system.
The data must then be processed to produce the final product, whether this is in the form of video with depth information or more complex 3D scans \cite{Li2017}.

\subsection{Datasets}

\subsubsection{Controlled Conditions Datasets}
The GRID dataset was released in 2006 \cite{Cooke2006} and contains 34,000 samples from 34 speakers, each with 1000 sentences.
The corpus uses sentences with a fixed grammar: 
<command:4>, <colour:4>, <preposition:4>, <letter:25>, <number:10>, <adverb:4>, with a total vocabulary of 51 words.
The primary limiting factor of the GRID dataset is that all data was captured directly for the use of the dataset.

\subsubsection{In The Wild Datasets}
To build larger datasets, subsequent datasets are commonly built with data "in the wild", meaning that the videos have not been captured with the intention of being used for this dataset.
Variations in lighting, angles, speakers and a wide vocabulary are common, this does however make the datasets more challenging to learn from.
LRW and LRS are both comprised of content from BBC broadcasts \cite{Chung2016, Chung2017} allowing for a far larger corpus size of over 1000 words for LRW and over 6000 words for LRS.
Both of datasets are captured and processed with the same pipeline summarised as follows. 
Firstly, as the subtitles are not aligned to the video on the broadcasts, optical chapter recognition is used to obtain the text being spoken in the video clips, the audio and text are then aligned per frame using the HTK toolkit \cite{Woodland1995}. 
A HOG-based detection algorithm \cite{King2009} is used for face detection for cropping the frames to the subjects head followed by facial landmarks for mouth localisation and speaker identification. 

LSVSR is a dataset published by DeepMind and Google which makes use of the huge amount of videos on YouTube \cite{Shillingford2018}, resulting in a total length of 3886 hours of training data.
The dataset is far larger than LRS, but aligns phonemes to frames, as opposed to words or characters.
The pre-processing steps are similar as in LRW and LRS; alignment is performed with the algorithm laid out in previous work by DeepMind \cite{Liao2013}, faces are tracked to ensure the speaker is visible in frame.

\subsection{3D Datasets} \label{3D Datasets}
To the best of the author's knowledge, there are currently two datasets with 3D temporal data which are appropriate for training lip reading models.
The first of which is LRW-3D \cite{Tzirakis2019} which has been captured from four subjects, two native English speakers and two non-native to increase variability in the dataset.
The subjects have been captured speaking the corpus used in the LRW dataset \cite{Chung2016}, a vocabulary of 500 words.
The resulting dataset comprises of 660 seconds of 3D meshes and audio per subject.
The dataset is not comprised of full sentences but would be appropriate for word-level lip reading.

The second dataset is the VOCASET \cite{Cudeiro2019}, captured from 6 male and 6 female subjects.
Each subject was recorded speaking 40 sequences, each ranging from 3 to 5 seconds, resulting in a total time of 30 minutes.
The recorded 3D meshes are registered to the FLAME model \cite{Li2017}, a statistical 3D facial mesh with around 5000 vertices.
Unlike the LRW-3D dataset, the sequences are grammatically correct sentences, chosen to maximise phonetic diversity.
This makes the VOCASET appropriate for creating a model for sentence-level lip reading.

It should be noted that neither of these datasets were captured for the purpose of lip reading, but for synthesising realistic statistical facial models driven from an audio input, and thus the transcriptions and frames are not aligned.
This alignment would have to be performed to the datasets before using the data to train lip reading models.
In order to achieve this automatic speech recognition (ASR) systems such as DeepSpeech \cite{Hannun2014} could be used to solve this problem.

\subsection{Lip Reading Models}
Chung et al \cite{Chung2016} used a convolutional model based on the VGG-M architecture which produced character level distributions with the LRW dataset, these distributions are then processed by a language model for text prediction.

The LipNet model \cite{Assael2016} was produced to be able to predict sentence-level lip reading of varied length, while previous work by \cite{Chung2016} predicted on a word level.
The model used spatiotemporal convolutions to process multiple frames of video at once followed by a recurrent layer using Gated Recurrent Units (GRU) \cite{Cho2014}.
The model used the GRID dataset \cite{Cooke2006} in which all subjects are recorded under consistent conditions of good lighting and fixed angles.
LipNet achieved the state of the art performance on the GRID dataset by using an model with increased complexity by incorporating 3D convolution and recurrent units.

As the LRW dataset could not be used to train a model such as LipNet for sentence-level lip reading, but the GRID dataset had limitations in the number of subjects and vocabulary, Chung et al created the LRS dataset \cite{Chung2017}.
The model presented was trained on both audio and video and made capable of taking either or both as the model inputs.
To prevent the model from being dependent on a single input source, the inputs are systematically distorted or removed.
The video input is passed through convolutional layers, followed by LSTM layers, while the audio is converted to MFCC, then input to LSTM layers.
The two are combined with then attention mechanism and further LSTM layers and an output fully connected layer with softmax activation for character distributions.
The model also makes use of curriculum learning by initially training the model on short sequences of single words.
The length of training sequences are increased throughout training.
It is stated by \cite{Chung2017} that this accelerates training and reduces overfitting.

In 2018 DeepMind published their V2P model \cite{Shillingford2018} along with the LSVSR dataset which is larger than all previous datasets, containing 3886 hours of training data.
The model follows a similar architecture to LipNet \cite{Assael2016} but with an increased number of convolutional layers and LSTM layers rather than GRUs.
It should be commented that due to the size of the model and dataset dictated the use of 64 GPUs for training to allow a batch size of 128.
Unlike previous models, the V2P model predicts phonemes as opposed to characters, these phonemes are then processed by a language model for word prediction as with previous models.

To the best of the author's knowledge, currently there do not exist any papers which have explored the use of 3D temporal datasets for the use with lip reading models. 
This is likely due to the shortage of 3D temporal data on which such models could be trained on.

\section{Data Generation}
In order to construct a deep learning lip reading model capable of being trained on 3D temporal data, appropriate datasets must be established.
Current datasets have been captured directly \cite{Tzirakis2019, Cudeiro2019} with the use of multi-camera capture rigs under controlled situations.
The total duration of both of these datasets is very short in comparison to the video datasets such as LRW, LRS and LSVSR, which is a limiting factor as to the models which could be trained using them.
As the models also use different mesh models to represent the data that has been captured this also prevents the two datasets being joined directly.
Unlike video data, there currently lacks a large body of 3D video data which is publicly available, limiting the construction of 3D datasets to directly capturing more 3D scans with multi-camera capture rigs, similar to those used in \cite{Tzirakis2019, Cudeiro2019} and generating synthetic training data.

\subsection{Audio Driven Data Generation}

\subsubsection{Imperial}
\cite{Tzirakis2019}

\subsubsection{VOCA}
The VOCA model \cite{Cudeiro2019} synthesises video sequences of 3D models speaking given an audio input.
The VOCASET dataset discussed in section \ref{3D Datasets}, was captured with the intention of training this model to be independent of the speaker, hence a large range of speakers are used within the dataset.
The model is comprised of three sections: audio feature extraction, a feature encoder and a decoder to drive a template facial mesh from the FLAME model \cite{Li2017}.
The audio feature extraction makes use of the pre-trained Mozilla implementation of the DeepSpeech model, based on the paper by Hannun et al. \cite{Hannun2014}.
The DeepSpeech model takes audio as an input and returns the unnormalised log-probabilities for an alphabet of the 26 standard characters, a space, apostrophe and blank character for time slices in the audio input.
The encoder is a convolutional network which is conditioned on the speakers identity, such that the latent space of speaker styles can later be explored on new audio inputs.
Finally, the decoder is made up of a fully connected layer with a linear activation function is used to output the displacements of the 5023 vertices in the template face.

\subsubsection{Nvidia}
\cite{Karras2017a}


\subsection{Adversarial Trained Generative Methods}
A recent development in generating synthetic data samples is the use of generative adversarial networks \cite{Goodfellow2014}.
The concept behind generative adversarial networks (GANs) is two have two machine learning models; a generator and a discriminator.
The task of the generator is to produce samples from an unknown high order probability distribution which correctly resemble samples from the distribution defined by training data.
The generator achieves this by transforming a random sample from a known probability distribution, such as a Gaussian distribution as used in \cite{Goodfellow2014}, to a sample from the unknown distribution.
This is achieved by finding the function which maps between the two distributions.
The discriminator however, attempts to correctly learn to discriminate between the real and the fake generated samples.

\begin{equation} \label{eq:gans_loss}
    \min_{G} \max_{D} V(G, D) = \mathbb{E}_{x \sim p_{data}(x)} [\log D(x)]
                              + \mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(x)))]
\end{equation}

\noindent The original loss function (\ref{eq:gans_loss}) proposed by Ian Goodfellow forms a min-max game, where the loss of the generator is attempting to be minimised by having the discriminator label all the generated samples as real.
While the loss of the discriminator is maximised by correctly classifying real and fake samples.
The two networks are trained in an alternating fashion until the discriminator achieves an accuracy of 50\%, effectively making binary guesses between real and generated samples.

GANs however, are difficult to train for two main reasons.
Firstly, the equation (\ref{eq:gans_loss}) is challenging as it provides small gradients while generated samples are poor as discussed in \cite{Goodfellow2014}, making training difficult.
Progress in developing new loss functions is discussed in section \ref{Stability_to_GANs}
Secondly, early networks must also be balanced with a similar model capacity to prevent one from getting too much better than the other, preventing the other from improving.
Various architectural changes have improved this issue \cite{Radford2016, Zhang2018}, although it seems to be closely tied to the loss function being used \cite{Gulrajani2017}.

\subsubsection{Stability Improvements of GANs Networks} \label{Stability_to_GANs}
There have been a large number of papers presenting new techniques with differing levels of success and training stability, a small handful of key papers which provided large advances in the generative adversarial training model shall be discussed here.
A primary research focus around GANs has been in finding new loss functions on which to train the model to improve stability and performance.
The original loss function proposed in the original paper \cite{Goodfellow2014} identifies an issue with equation (\ref{eq:gans_loss}) in that the gradient back propagated to the generator when the generated samples are poor, is very low as shown in figure \ref{fig:Goodfellow_plot}.

\begin{figure}[h]
    \centering
        \includegraphics[width=0.8\textwidth]{../figures/goodfellow_gen_losses.png}
    \caption{Goodfellow's Generator Loss Plots \cite{Goodfellow2014}}\label{fig:Goodfellow_plot}
\end{figure}

\noindent This in turn makes it very challenging to improve the performance of the generator.
The suggested improvement made in \cite{Goodfellow2014} is rather than to maximise the the number of generated samples which the discriminator incorrectly classifies, but to minimise the number of generator samples the discriminator correctly classifies, described in equation (\ref{eq:gans_loss2}).
Figure \ref{fig:Goodfellow_plot} shows how this results in the gradient propagated back to the generator is far larger when the generated samples are poor.

\begin{equation} \label{eq:gans_loss2}
    \min_{G} \max_{D} V(G, D) = \mathbb{E}_{x \sim p_{data}(x)} [\log D(x)]
                              - \mathbb{E}_{z \sim p_{z}(z)} [\log (D(G(x)))]
\end{equation}

Further stability improvements were proposed in \cite{Radford2016} which allowed deep convolutional generative adversarial networks (DCGANs) to be successfully trained for the first time.
Radford et al. proposed three main contributions.
Replace deterministic pooling layers with strided convolutions in both the generator and discriminator networks, allowing the networks to learn their own spatial upsampling and downsampling.
Remove all fully connected layers used on top of convolutional layers, resulting in a fully convolutional model.
Apply batch normalisation \cite{Ioffe2015} before the input of each layer.
This normalises the input to each unit to zero mean and unit variance.
This assists with training problems due to poor weight initialisation and allows gradients to flow through deeper networks more easily.
Radford et al. state this to be a critical improvement to allow generator networks to begin learning by preventing all samples from collapsing to a single point. 

In an attempt to stabilize the training of GANs models, the Wasserstein or 'Earth-Mover' loss function was proposed by Arjovsky et al. \cite{Arjovsky2017}.
By using the Wasserstein loss function \ref{eq:wgan}, the generator and discriminator no longer have to alternate training epochs to try to maintain model stability, but can both be trained to saturation in a loop.

\begin{equation} \label{eq:wgan}
    \min_{G} \max_{D} V(G, D) = \mathbb{E}_{x \sim p_{data}(x)} [\log D(x)]
                              + \mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(x)))]
\end{equation}

\noindent By using the Wasserstein loss function, the discriminator, or critic as it is named by Arjovsky et al., is trained to an optimum point, at which time the loss provided to the generator is at a maximum, making training the generator easier.
The discriminator is referred to as the critic as it is no longer attempting to classify samples are real or fake.
The generator can then be trained to an optimum position, at which time, training the critic can continue again.
This method has greatly improved stability and is fairly robust to a large range of model architectures which have been tested.
The generator and critic also no longer have to have a similar capacity.

To achieve this, the Lipschtiz condition must be met.
This is achieved by clipping the weights on the model.
The paper points out that this method of achieving the Lipschtiz condition is non-ideal and is a point of further work.

Other points of note, the loss plot produced by the WGAN model seems to correlate with perceived image quality, which could not be said for precious loss functions.

\subsubsection{WGAN}
\subsubsection{WGAN+GP}

\subsubsection{Architectural Developments}
\begin{itemize}
    \item Conditional GANs
    \item DCGAN
    \item Temporal GANs
    \item Attention GANs
\end{itemize}


\section{3D Modelling}
\begin{itemize}
    \item blendshapes
    \item Model Correspondence
\end{itemize}

\section{Recurrent Models}
\begin{itemize}
    \item Recurrent networks
    \item LSTMs
    \item Attention Mechanism
\end{itemize}


\section{Speech Recognition}
Deep Speech \cite{Karras2017a}

\section{Current Work and Future Work}
Data collection

\bibliographystyle{unsrt}
\bibliography{../ref}
\end{document}
