\documentclass[12pt]{article}

\usepackage[nottoc,notlot,notlof]{tocbibind}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{geometry}
 \geometry
 {
     a4paper,
     left=25mm,
     right=25mm,
     top=25mm,
     bottom=25mm,
 }

\title{MSc Background Report}
\author{Peter Robertson}
\date{}

\begin{document}
\maketitle

% page numbering etc.
\pagenumbering{roman}
\setcounter{page}{0}
\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\tableofcontents 
\clearpage{\pagestyle{empty}\cleardoublepage}


\pagenumbering{arabic}
\setcounter{page}{1}

\section{Introduction}

\textbf{( I want to rewrite this )}

Current methods of visual speech prediction, often referred to as lip reading, focuses on training machine learning models on 2D temporal data of participants speaking, labelled with the accompanying spoken text \cite{Chung2016, Assael2016, Chung2017, Shillingford2018}.
However there has been little progress in lip reading from 3D temporal models which contain depth information of the subject.
A substantial barrier to this problem is the lack of 3D datasets of subjects speaking when compared to the relative abundance of video datasets.
To first attempt this problem, new datasets which capture 3D temporal models of subjects heads speaking must be established by the community.

\section{Lip Reading}
In recent years the problem of Lip Reading has seen huge advances due to the availability of new datasets and the use of Deep Learning models.
In 2016 the Lip Reading Words (LRW) dataset was published \cite{Chung2016}, followed by Lip Reading Sentences (LRS) in 2017 \cite{Chung2017} and later Large-Scale Visual Speech Recognition (LSVSR) in 2018 \cite{Shillingford2018}, each of which larger in size and resulted in a model which outperforms previous models.
Chung et al \cite{Chung2016} used a convolutional model based on the VGG-M architecture which produced character level distributions with the LRW dataset, these distributions are then processed by a language model for text prediction.
The LRW dataset contains over 1000 hours of spoken text taken from BBC broadcasts, with a over 1000 different speakers, variations in angles and lighting, a vocabulary size of over 1000 words and over one million word utterances.

The LipNet model \cite{Assael2016} was produced to be able to predict sentence-level lip reading of varied length, while previous work by \cite{Chung2016} predicted on a word level.
The model used spatiotemporal convolutions to process multiple frames of video at once followed by a recurrent layer using Gated Recurrent Units (GRU) \cite{Cho2014}.
The model used the GRID dataset \cite{Cooke2006} which is of a similar size as LRW, but has far less variation in speakers.
LipNet achieved the state of the art performance on the GRID dataset by using an model with increased complexity by incorporating 3D convolution and recurrent units.

As the LRW dataset could not be used to train a model such as LipNet for sentence-level lip reading, but the GRID dataset had limitations in the number of subjects, Chung et al created the LRS dataset \cite{Chung2017}.
The model presented was trained on both audio and video and made capable of taking either or both as the model inputs.
To prevent the model from being dependent on a single input source, the inputs are systematically distorted or removed.
The video input is passed through convolutional layers, followed by LSTM layers, while the audio is converted to MFCC, then input to LSTM layers.
The two are combined with then attention mechanism and further LSTM layers and an output fully connected layer with softmax activation for character distributions.
The model also makes use of curriculum learning by initially training the model on short sequences of single words.
The length of training sequences are increased throughout training.
It is stated by \cite{Chung2017} that this accelerates training and reduces overfitting.

In 2018 DeepMind published their V2P model \cite{Shillingford2018} along with the LSVSR dataset which is larger than all previous datasets, containing 3886 hours of training data.
The model follows a similar architecture to LipNet \cite{Assael2016} but with an increased number of convolutional layers and LSTM layers rather than GRUs.
The size of the model and dataset dictated the use of 64 GPUs for training to allow a batch size of 128.
Unlike previous models, the V2P model predicts phonemes as opposed to characters, these phonemes are then processed by a language model for word prediction as with previous models.


\section{Data Generation}
\subsection{Data Capture}
Current lip reading models currently make use of 2D temporal data such as videos.
This is convenient as video data is already widely available and relatively straightforward to process.
The reason for the wide availability of video data is due to the ease of capturing and viewing such data.
Unlike 2D temporal data, 3D temporal data or 3D video, requires multiple cameras to record simultaneously, which requires synchronisation, adding to the complexity of the system.
The data must then be processed to produce the final product, whether this is in the form of video with depth information or more complex 3D scans \cite{Li2017}.

\subsubsection{2D Datasets}
Due to the large availability of video data, datasets such as GRID \cite{Cooke2006}, LRW \cite{Chung2016}, LRS \cite{Chung2017} and LSVSR \cite{Shillingford2018} have been constructed by compiling the appropriate data.
GRID contains 34,000 samples from 34 speakers, each with 1000 sentences.
The corpus uses sentences with a fixed grammar: 
<command:4>, <colour:4>, <preposition:4>, <letter:25>, <number:10>, <adverb:4>, with a total vocabulary of 51 words.

LRW and LRS are both comprised of content from BBC broadcasts \cite{Chung2016, Chung2017} allowing for a far larger corpus size of over 1000 words for LRW and over 6000 words for LRS.
Both of datasets are captured and processed with the same pipeline summarised as follows. 
Firstly, as the subtitles are not aligned to the video on the broadcasts, optical chapter recognition is used to obtain the text being spoken in the video clips, the audio and text are then aligned per frame using the HTK toolkit \cite{Woodland1995}. 
A HOG-based detection algorithm \cite{King2009} is used for face detection for cropping the frames to the subjects head followed by facial landmarks for mouth localisation and speaker identification. 
\subsubsection{3D Datasets}

\subsubsection{Original Dataset}

\section{Statistical Models}
\begin{itemize}
    \item Audio driven models
\end{itemize}

\section{Generative Models}
\begin{itemize}
    \item Intro to GANs
    \item Issues with GANs
    \item Progression of GANs
    \item GANs for synthesising 2D lip sync video
\end{itemize}

\section{3D Modelling}
\begin{itemize}
    \item blendshapes
    \item Model Correspondence
\end{itemize}

\section{Recurrent Models}
\begin{itemize}
    \item Recurrent networks
    \item LSTMs
    \item Attention Mechanism
\end{itemize}


\section{Speech Recognition}
Deep Speech \cite{Karras2017a}

\section{Current Work and Future Work}
Data collection

\bibliographystyle{unsrt}
\bibliography{../ref}
\end{document}
